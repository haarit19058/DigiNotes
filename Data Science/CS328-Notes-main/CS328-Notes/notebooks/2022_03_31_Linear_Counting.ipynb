{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the streaming problem: Distinct Count\n",
    "\n",
    "The question we are trying to answer is the number of distinct elements in a stream of elements. The naive approach to solve this problem can be using a set or hashmap implementation where we store the frequency of elements against their values as keys. However, this is a space consuming solution O(n*$log(u)$). There can be some more naive and space consuming methods as discussed in the previous lecture notes.\n",
    "\n",
    "To optimize for space, we can incorporate a small tradeoff of the \"exactness\" of the solution. Hence, we want to try to do this in less space when the exact solution is not needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Counting embraces hash collisions and doesn't store the exact original items. \n",
    "\n",
    "* It allocates a hash table/ bit array B of size $m$ bits where $m$ is the same scale as $n$, the number of unique items. All these $m$ bits are initialized to 0. \n",
    "* hash function h: [n] --> [m]\n",
    "* Now, when we encounter any element x in the stream, set B[h(x)] = 1, i.e, set the bit returned by h(x) to 1.\n",
    "\n",
    "Then, we want to look at the number of zero entries - $Z_m$.\n",
    "\n",
    "return $ \\hat{n} = - m\\text{ }ln (\\frac{Z_m}{m}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Counting Analysis\n",
    "\n",
    "* Probability that any paritcular bit position remains 0 is the probability that all the insertions went to the remaning m - 1 bits. \\\n",
    "\n",
    "  $P_r$ [position remaining 0] = $ (1 - \\frac{1}{m})^n \\approx e^{-\\frac{n}{m}}$\n",
    "* Expected number of positions at 0: $E[Z_m]$ = $ m e^{-\\frac{n}{m}}$ \\\n",
    " because the probability that a single one remains at zero was $e^{-\\frac{n}{m}}$ and taking this indicated random variable and this expectation turns out to be $ m e^{-\\frac{n}{m}}$\n",
    "\n",
    "* suppose $ \\hat{Z_m} $ is close to its expectation, then \n",
    "  * $ \\hat{Z_m} \\approx m e^{-\\frac{n}{m}}$. \n",
    "  * Thus, $ ln(\\frac{\\hat{Z_m}}{m}) \\approx -\\frac{n}{m} $\n",
    "  * $ \\hat{n} = - m\\text{ }ln (\\frac{Z_m}{m}) $.\n",
    "  * This equality (concentration around the estimate) can be easily shown using tail inequalities. For the same to be useful in practice, we need $m$ to be some constant factor of $n$, i.e, O($n$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<footer>\n",
    "    Author(s): Eshan Gujarathi, Hitarth Gandhi, Vishal Soni\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
