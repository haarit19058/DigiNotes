{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Valued Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Introduction</b></h3>\n",
    "\n",
    "The Singular Value Decomposition (SVD) is a basic tool frequently used in\n",
    "Numerical Linear Algebra and in many applications, which generalizes the\n",
    "Spectral Theorem from symmetric n √ó n matrices to general m √ó n matrices.\n",
    "We introduce the reader to some of its beautiful properties, mainly related to\n",
    "the Eckart-Young Theorem, which has a geometric nature. The implementation of a SVD algorithm in the computer algebra software Macaulay2 allows\n",
    "a friendly use in many algebro-geometric computations.\n",
    "\n",
    " Let A be an n √ó d matrix  in d-dimensional space. \n",
    " Consider where subspace is 1-d dimensional \n",
    " For k = 1, 2, 3,..., for the collection of n data points, the singular value decomposition yields the best-fitting k-dimensional subspace.</b>\n",
    "\n",
    "Let's start with a one-dimensional subspace, such as a line through the origin.\n",
    "Next the best-fitting k-dimensional subspace may be discovered by k applications of the best fitting line approach, where the best fit line perpendicular to the preceding i-1 lines is found on the i th iteration.When k approaches the rank of the matrix, we have a precise breakdown of the matrix termed the singular value decomposition from these processes.</b>\n",
    "\n",
    "SVD of  a matrix A with real elements is the factorization of A into the product of three matrices, A = UDV T, where the columns of U and V are orthonormal and the matrix D is diagonal with positive real values. The unit length vectors are represented by V's columns. The coordinates of a row of U will be fractions of the corresponding row of A along each of the lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Explanation</b></h3>\n",
    "\n",
    "\n",
    "\n",
    "Assume that we have a matrix A (nxm) with rank r. Dimensions of rowspace and column space can be atmost n, m respectively. However as the rank of A is r there are excatly r linearly independent row vectors and column vectors, thus the dimension of rowspace and colspace is r. \n",
    "\n",
    "<b>This means that there are r basis vectors for the rowspace and columnspace.</b>\n",
    "\n",
    "```{note} these basis vectors of rowspace, column space can be found out from the  corresponding echelon matrix using non-zero rows and pivot columns.\n",
    "\n",
    "```\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Matrix can be viewed as an operator of linear transformation, each column of a matrix represents a trasformed basis vector. let v be a vector in rowspace of A, then A*v gives a vector in the column space of A. i.e. A v=œÉ u, œÉ is a constant.\n",
    "\n",
    "For the rowspace,there exists some orthogonal basis (say v). we are intrested in finding such basis that gets transformed by A to an orhtogonal basis in colspace.\n",
    "\n",
    "```{note} For a subspace formed by span of vectors, there can be multiple sets of basis vectors. Using algorithm given by Gram-schmidt, an orthogonal basis vectors for the same space can be found using some set of basis vectors.\n",
    "\n",
    "```\n",
    "\n",
    " \n",
    "\n",
    "let v1, v2, v3... vr be that orthogonal basis vectors of the rowspace. u1, u2, u3,...,ur be the transformed vectors that form an orhtogonal basis of the column space,<br><br>\n",
    " i.e. <br><br><br>\n",
    "\n",
    "<!-- <img src=\"../assets/2022_3_14_svd/svd_0.png\"> -->\n",
    "![](../assets/2022_3_14_svd/svd_0.jpg)\n",
    "\n",
    "\n",
    "```{note} generally in this Œ£ matrix, œÉ values are kept in decreasing order, as a consequence the order of v vectors in V and u vectors in V is also changed to maintain the correspondence.\n",
    "\n",
    "```\n",
    " \n",
    " \n",
    "<br>\n",
    "<b>A V=U ùö∫</b>\n",
    "<br>multiply by V<sup>  -1</sup> on both sides \n",
    "<br>A V V <sup>  -1</sup> =U ùö∫ V<sup>  -1</sup>\n",
    "<br> A = U ùö∫ V<sup>  -1</sup>\n",
    "<br> U and V are the collection of orthogonal basis vectors, so U and V are othrogonal matrices. So V V<sup>T</sup> = I, i.e. V<sup>-1</sup> = V<sup>T</sup>\n",
    "<br> <b>A = U ùö∫ V<sup>  T</sup></b>\n",
    "\n",
    "if we generalize this equation considering all vectors of row and column space, we get\n",
    "<br><br>\n",
    "\n",
    "<!-- <img src=\"../assets/2022_3_14_svd/svd_3.png\" > -->\n",
    "![](../assets/2022_3_14_svd/svd_3.jpg)\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    " Here u<sub>i</sub>'s are called left singluar vectors and v<sub>i</sub>'s are called right singular vectors and œÉ<sub>i</sub>'s are called singular \n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "<br>\n",
    "\n",
    "```{note} when the rank of A is r ( r <= min(n,m)) then we need only first r vectors of U, V. we are able to take only these vectors by manipulating zeroes in matrix ùö∫.\n",
    "\n",
    "```\n",
    " \n",
    "\n",
    "\n",
    "<br><br>\n",
    "Now let us consider A A<sup>  T</sup>\n",
    "<br> A A<sup>  T</sup> = (U ùö∫ V<sup>  T</sup>) (U ùö∫ V<sup>  T</sup>)<sup>T</sup>\n",
    "<br>= U ùö∫ V<sup>  T</sup> V</sup> ùö∫<sup>T</sup> U<sup>T</sup>\n",
    "<br>\n",
    "as ùö∫ is a diagnol matrix ùö∫<sup>T</sup>=ùö∫\n",
    "<br>= U ùö∫ <sup>2</sup> U<sup>T</sup>\n",
    "\n",
    "<b>Thus u<sub>i</sub>'s  are eigen vectors of A A<sup>T</sup> and œÉ<sub>i</sub>'s are the square roots of eigen values of A A<sup>T</sup></b>\n",
    "\n",
    "<br>\n",
    "\n",
    "Now let us consider A<sup>  T</sup> A\n",
    "\n",
    "A<sup>  T</sup> A = (U ùö∫ V<sup>  T</sup>)<sup>T</sup> (U ùö∫ V<sup>  T</sup>)\n",
    "<br> = V ùö∫<sup>T</sup> U <sup>T</sup> U ùö∫ V<sup>  T</sup>\n",
    "<br> = V ùö∫<sup>T</sup> U <sup>T</sup> U ùö∫ V<sup>  T</sup>\n",
    "<br> = V ùö∫<sup>2</sup> V<sup>T</sup>\n",
    "\n",
    "\n",
    "<b>Thus v<sub>i</sub>'s  are eigen vectors of A<sup>T</sup> A and œÉ<sub>i</sub>'s are the square roots of eigen values of A<sup>T</sup> A</b>\n",
    "\n",
    " ***Property: Matrices (AB) and (BA) have same eigen values.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "<h3><b>Top K Truncated SVD:</b></h3>\n",
    "\n",
    "use first k vectors of U, V and the corresponding spectral values (top k x k submatrix of Œ£ )  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Implementation of SVD</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.69366543,  0.59343205, -0.40824829],\n",
       "        [-0.4427092 , -0.79833696, -0.40824829],\n",
       "        [-0.56818732, -0.10245245,  0.81649658]]),\n",
       " array([10.25142677,  2.62835484]),\n",
       " array([[-0.88033817, -0.47434662],\n",
       "        [ 0.47434662, -0.88033817]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[7, 2], [3, 4], [5, 3]])\n",
    "U, D, V = np.linalg.svd(A)  # actually V here = V (Transpose) in svd\n",
    "U, D, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- <h3><b> Geometrical view of svd </b></h3> -->\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "<!-- <img src=\"../assets/2022_3_14_svd/goe.jpg\" width=\"550px\" height=\"400px\"> -->\n",
    "\n",
    "<!-- ![](..\\assets\\2022_3_14_svd\\goe.jpg) -->\n",
    "\n",
    "![](../assets/2022_3_14_svd/goe.jpg)\n",
    "\n",
    "<!-- ```{figure} ../assets/2022_3_14_svd/goe.jpg\n",
    ":height: 150px\n",
    "``` -->\n",
    "\n",
    "<!-- ```{image} ../assets/2022_3_14_svd/goe.jpg\n",
    ":height: 150px\n",
    ":name: image-example\n",
    "``` -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Fig credit: Math for CSLecture\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The above image shows how the points get linearly transformed by A, i.e. A v= œÉ u (equation 1)\n",
    "<br> \n",
    "svd says 3 important things about this transormation\n",
    "\n",
    "1.  The u<sub>i</sub> vectors in SVD decompostion of A form the axes of the new  transformed subspace\n",
    "2. the length of each axis is equal to coressponding œÉ<sub>i</sub> value\n",
    "3. the preimage of u<sub>i</sub> is the corresponding v<sub>i</sub> of the SVD of A (from equation 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>References</b>:\n",
    "\n",
    "https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-29-singular-value-decomposition/\n",
    "\n",
    "\"Spectral Learning on Matrices and Tensors\" textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<footer> <i><b>Authors: Binita, GV Sai, Sai Narasimha</b></i></footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e73adf70cdfb7bfa2b51882d5ba08fc34d4f5d14a19411ced4c80d8ca411f609"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
